# üìÅ Project Structure Guide

## Complete Folder Layout

```
ai-ecommerce-fraud-detection/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ data/                          # All dataset files
‚îÇ   ‚îú‚îÄ‚îÄ raw/                          # Original, unprocessed data
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ creditcard.csv            # Kaggle fraud dataset (280+ MB)
‚îÇ   ‚îú‚îÄ‚îÄ processed/                    # Cleaned, preprocessed data
‚îÇ   ‚îî‚îÄ‚îÄ sample_transactions.csv       # Sample data for testing
‚îÇ
‚îú‚îÄ‚îÄ üìÇ notebooks/                     # Jupyter notebooks for exploration
‚îÇ   ‚îú‚îÄ‚îÄ 01_EDA.ipynb                  # Exploratory Data Analysis
‚îÇ   ‚îî‚îÄ‚îÄ 02_Model_Training.ipynb       # Model training experiments
‚îÇ
‚îú‚îÄ‚îÄ üìÇ src/                           # Source code (Python modules)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                   # Package initialization
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py                # Load and validate datasets
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py              # Feature engineering, scaling, SMOTE
‚îÇ   ‚îú‚îÄ‚îÄ train_model.py                # Train and save ML models
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_model.py             # Model evaluation with visualizations
‚îÇ   ‚îú‚îÄ‚îÄ predict.py                    # Make predictions on new data
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                      # Helper functions
‚îÇ
‚îú‚îÄ‚îÄ üìÇ models/                        # Saved ML models
‚îÇ   ‚îú‚îÄ‚îÄ fraud_detector.pkl            # Trained model
‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl                    # Feature scaler
‚îÇ   ‚îú‚îÄ‚îÄ feature_names.pkl             # List of features
‚îÇ   ‚îî‚îÄ‚îÄ training_history.pkl          # Training metadata
‚îÇ
‚îú‚îÄ‚îÄ üìÇ api/                           # REST API (optional)
‚îÇ   ‚îî‚îÄ‚îÄ app.py                        # Flask API server
‚îÇ
‚îú‚îÄ‚îÄ üìÇ docs/                          # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ ABSTRACT.md                   # Academic abstract
‚îÇ   ‚îú‚îÄ‚îÄ PROJECT_PROPOSAL.md           # Project proposal
‚îÇ   ‚îú‚îÄ‚îÄ EXECUTION_GUIDE.md            # Complete execution guide
‚îÇ   ‚îú‚îÄ‚îÄ QUICKSTART.md                 # Quick setup guide
‚îÇ   ‚îî‚îÄ‚îÄ DELIVERY_SUMMARY.md           # Submission summary
‚îÇ
‚îú‚îÄ‚îÄ üìÑ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ üìÑ README.md                      # Main project documentation
‚îú‚îÄ‚îÄ üìÑ .gitignore                     # Git ignore rules
‚îî‚îÄ‚îÄ üìÑ PROJECT_STRUCTURE.md           # This file

```

---

## üìÇ Folder Explanations

### 1Ô∏è‚É£ `data/` - Dataset Storage

**Purpose:** Store all data files (raw and processed)

**Subfolders:**
- **`raw/`** - Original datasets (never modify these!)
  - `creditcard.csv` - Main Kaggle dataset
  - Downloaded from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
  - Size: ~150 MB (284,807 transactions)

- **`processed/`** - Cleaned/transformed data
  - Created by preprocessing scripts
  - Split datasets (train/test)
  - Feature-engineered data

- **`sample_transactions.csv`** - Small dataset for testing
  - 30 sample transactions
  - Use when main dataset is unavailable

**Why separate raw and processed?**
- Raw data = source of truth (never modify)
- Processed data = intermediate results (can regenerate)

---

### 2Ô∏è‚É£ `notebooks/` - Interactive Exploration

**Purpose:** Jupyter notebooks for analysis and experimentation

**Files:**
- **`01_EDA.ipynb`** - Exploratory Data Analysis
  - Understand data distribution
  - Visualize fraud vs genuine patterns
  - Identify important features
  - Check for missing values/outliers

- **`02_Model_Training.ipynb`** - Model Development
  - Train different ML models
  - Compare performance
  - Tune hyperparameters
  - Save best model

**How to use:**
```bash
# Start Jupyter
jupyter notebook

# Navigate to notebooks/ and open files
# Run cells with Shift+Enter
```

**Why notebooks?**
- Interactive experimentation
- Visualizations
- Step-by-step execution
- Easy to share insights

---

### 3Ô∏è‚É£ `src/` - Production Code

**Purpose:** Reusable, production-ready Python modules

**Files:**

#### **`__init__.py`**
- Makes `src/` a Python package
- Allows imports: `from src.data_loader import FraudDataLoader`

#### **`data_loader.py`**
- Loads datasets (Kaggle or sample)
- Validates data structure
- Creates synthetic data if needed

**Example:**
```python
from src.data_loader import FraudDataLoader

loader = FraudDataLoader()
df = loader.load_creditcard_data()
```

#### **`preprocessing.py`**
- Feature scaling (StandardScaler)
- Train-test split (80-20)
- SMOTE for class balancing
- Feature engineering

**Example:**
```python
from src.preprocessing import FraudPreprocessor

preprocessor = FraudPreprocessor(test_size=0.2)
data = preprocessor.full_preprocessing_pipeline(df)
```

#### **`train_model.py`**
- Train ML models (Logistic Regression, Random Forest)
- Evaluate performance
- Save trained models

**Usage:**
```bash
python src/train_model.py
```

#### **`evaluate_model.py`**
- Generate classification reports
- Plot confusion matrix
- ROC curve visualization
- Precision-Recall curves

**Usage:**
```bash
python src/evaluate_model.py
```

#### **`predict.py`**
- Make predictions on new transactions
- Interactive mode (user input)
- Batch mode (CSV file)
- API integration

**Usage:**
```bash
python src/predict.py --demo
```

#### **`utils.py`**
- Helper functions
- Logging utilities
- File path management

---

### 4Ô∏è‚É£ `models/` - Saved ML Models

**Purpose:** Store trained models for deployment

**Files:**
- **`fraud_detector.pkl`** - Main trained model
- **`scaler.pkl`** - Feature scaler (fit on training data)
- **`feature_names.pkl`** - List of feature names (order matters!)
- **`training_history.pkl`** - Training metadata (date, metrics, etc.)

**Why save models?**
- No need to retrain every time
- Fast predictions
- Version control for models

**How to use:**
```python
import joblib

# Load model
model = joblib.load('models/fraud_detector.pkl')
scaler = joblib.load('models/scaler.pkl')

# Make prediction
prediction = model.predict(scaled_features)
```

---

### 5Ô∏è‚É£ `api/` - REST API (Optional)

**Purpose:** Expose model as web service

**Files:**
- **`app.py`** - Flask REST API

**Endpoints:**
- `GET /` - API documentation
- `GET /health` - Health check
- `POST /predict` - Make prediction
- `POST /predict/batch` - Batch predictions

**Usage:**
```bash
python api/app.py

# API runs on http://localhost:5000
```

**Why API?**
- Real-world integration
- Microservice architecture
- Remote predictions
- Platform-independent

---

### 6Ô∏è‚É£ `docs/` - Documentation

**Purpose:** All project documentation

**Files:**
- **`ABSTRACT.md`** - Academic abstract (IEEE format)
- **`PROJECT_PROPOSAL.md`** - Initial project proposal
- **`EXECUTION_GUIDE.md`** - Complete step-by-step guide
- **`QUICKSTART.md`** - Quick setup (5 minutes)
- **`DELIVERY_SUMMARY.md`** - Submission checklist

---

## üìÑ Root Files

### **`requirements.txt`**
Lists all Python dependencies

```txt
pandas>=1.5.0
numpy>=1.23.0
scikit-learn>=1.2.0
imbalanced-learn>=0.10.0
matplotlib>=3.6.0
seaborn>=0.12.0
jupyter>=1.0.0
flask>=2.3.0
joblib>=1.2.0
```

**Install:**
```bash
pip install -r requirements.txt
```

---

### **`README.md`**
Main project documentation

**Sections:**
- Project overview
- Installation instructions
- Usage examples
- Tech stack
- Team information

---

### **`.gitignore`**
Tells Git which files NOT to track

**Ignores:**
- Large datasets (`*.csv`, `data/raw/`)
- Trained models (`*.pkl`)
- Python cache (`__pycache__/`, `*.pyc`)
- Virtual environments (`.venv/`, `venv/`)
- Jupyter checkpoints (`.ipynb_checkpoints/`)
- IDE files (`.vscode/`, `.idea/`)

**Why?**
- Keep repository small
- Avoid committing sensitive data
- Don't track generated files

---

## üîÑ Typical Workflow

### Step 1: Setup
```bash
# Clone repository
git clone https://github.com/yagneshj4/ai-ecommerce-fraud-detection.git
cd ai-ecommerce-fraud-detection

# Install dependencies
pip install -r requirements.txt

# Download dataset to data/raw/
```

### Step 2: Exploration
```bash
# Open Jupyter
jupyter notebook

# Run 01_EDA.ipynb
# Understand data patterns
```

### Step 3: Training
```bash
# Train model
python src/train_model.py

# Models saved to models/
```

### Step 4: Evaluation
```bash
# Evaluate model
python src/evaluate_model.py

# Generates plots and metrics
```

### Step 5: Prediction
```bash
# Interactive mode
python src/predict.py

# Or batch mode
python src/predict.py --batch data/sample_transactions.csv
```

### Step 6: API (Optional)
```bash
# Start API server
python api/app.py

# Test at http://localhost:5000
```

---

## üéØ Best Practices

### 1. **Never modify `data/raw/`**
- Raw data = source of truth
- Always work with copies

### 2. **Use meaningful file names**
- ‚úÖ `fraud_detector_v2.pkl`
- ‚ùå `model.pkl`

### 3. **Comment your code**
```python
# Good comment - explains WHY
# Using SMOTE because dataset is imbalanced (0.17% fraud)
smote = SMOTE()
```

### 4. **Version control your code, not data**
- Commit: `.py`, `.ipynb`, `.md`
- Don't commit: `.csv`, `.pkl`, `__pycache__/`

### 5. **Test before pushing**
```bash
# Run tests
python src/train_model.py

# Check for errors
# Then push to GitHub
git push
```

---

## üìä File Size Guide

| Folder | Typical Size | In Git? |
|--------|--------------|---------|
| `data/raw/` | 150 MB | ‚ùå No |
| `data/processed/` | 50 MB | ‚ùå No |
| `models/` | 10 MB | ‚ùå No |
| `notebooks/` | 5 MB | ‚úÖ Yes |
| `src/` | 100 KB | ‚úÖ Yes |
| `api/` | 20 KB | ‚úÖ Yes |
| `docs/` | 200 KB | ‚úÖ Yes |

**Total Git Repo:** ~5-10 MB  
**Total Project (with data):** ~200+ MB

---

## üöÄ Quick Commands

```bash
# Setup
pip install -r requirements.txt

# Train model
python src/train_model.py

# Make predictions
python src/predict.py --demo

# Start API
python api/app.py

# Open notebooks
jupyter notebook

# Run evaluation
python src/evaluate_model.py
```

---

**Team Three Unknowns - VRSEC**  
*Last Updated: January 2, 2026*
